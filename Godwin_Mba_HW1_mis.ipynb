{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe5-Zio5zLSR"
      },
      "source": [
        "Godwin Mba\n",
        "\n",
        "\n",
        "\n",
        "gomb0114"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7vNf-yPJOx75"
      },
      "source": [
        "# Homework 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKdfRsPZOx7-"
      },
      "source": [
        "This homework evaluates if you are familar with NumPy to create machine learning related functions. Thus, all your functions here should be only made using NumPy, without using any other libraries like pandas, scikit-learn, or scipy. You will run a complete machine learning process from preprocessing to model performance testing by using the functions you will develop.\n",
        "\n",
        "### This assignment is made to practice the features of NumPy. Therefore, the use of for loops is strictly prohibited. All problems can be solved without explicit loops. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9BPzS0g9Ox7-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svj3G7hj4ZxY"
      },
      "source": [
        "In this homework, you will continue to use the same dataset you have used in the first lab, as well as the code you have made. However, this time, you will create a model and even test it - by dividing the dataset into training and test sets. You will practice applying each function one after the other in a correct order. \n",
        "\n",
        "To practice NumPy efficiently, unlike the lab, you will only use the values of the dataset, ignoring Pandas properties such as column names.\n",
        "\n",
        "- Run the block below to load the dataset (**X** and **y**)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2HAUc00Wmu0E"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "california_housing = fetch_california_housing(as_frame=False)\n",
        "\n",
        "X = california_housing.data\n",
        "y = california_housing.target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-txQY1eUoWeg"
      },
      "source": [
        "Now, you are ready to use both features and labels. To handle it correctly, you need to be familiar with its axis concept as it no longer has indices and columns that you can check by printing the variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4KbMNxMm9Oi",
        "outputId": "32b5db32-ca09-49bb-e90b-24277356eb43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[   8.3252    ,   41.        ,    6.98412698, ...,    2.55555556,\n",
              "          37.88      , -122.23      ],\n",
              "       [   8.3014    ,   21.        ,    6.23813708, ...,    2.10984183,\n",
              "          37.86      , -122.22      ],\n",
              "       [   7.2574    ,   52.        ,    8.28813559, ...,    2.80225989,\n",
              "          37.85      , -122.24      ],\n",
              "       ...,\n",
              "       [   1.7       ,   17.        ,    5.20554273, ...,    2.3256351 ,\n",
              "          39.43      , -121.22      ],\n",
              "       [   1.8672    ,   18.        ,    5.32951289, ...,    2.12320917,\n",
              "          39.43      , -121.32      ],\n",
              "       [   2.3886    ,   16.        ,    5.25471698, ...,    2.61698113,\n",
              "          39.37      , -121.24      ]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rTO_NYMNHlg",
        "outputId": "9a82848d-c45b-4ae8-e266-ee697ace4bb8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4.526, 3.585, 3.521, ..., 0.923, 0.847, 0.894])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fbj_IpkHOx8A"
      },
      "source": [
        "### 1. Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE0ALwTyOx8A"
      },
      "source": [
        "The first task is to open the dataset and preprocess it into the form that the model can understand. It involves imputation, train_test_split, standardization, and normalization. Some functions are already covered by the first lab, so if you finished the lab before, you can freely bring your code here to finish your homework.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2uFZSUuoh9t"
      },
      "source": [
        "First, you need to develop both standardization and normalization functions. You can re-use your lab functions here if you have finished your lab tasks. Please carefully refer to the definitions of those two functions as follows:\n",
        "\n",
        "\n",
        "- Standardization: Make features have the same standard deviaton and mean.\n",
        "\n",
        "- Normalization: Make the range of value normalized into [0, 1]. This means that each column's minimum value should be zero and maximum value should be one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "bQRVVievOx8A"
      },
      "outputs": [],
      "source": [
        "# def standardize(data):\n",
        "#   \"\"\"\n",
        "#   Input: NumPy ndarray\n",
        "#   Output: NumPy ndarray with column mean == 0 and std == 1\n",
        "#   \"\"\"\n",
        "#   x_data = np.array(data)\n",
        "  \n",
        "#   standard_data = (x_data - np.mean(x_data)) / (np.std(x_data))\n",
        "\n",
        "#   return standard_data \n",
        "\n",
        "# def normalize(data):\n",
        "#   \"\"\"\"\n",
        "#   Input: NumPy ndarray\n",
        "#   Output: NumPy ndarray with column min == 0 and max == 1\n",
        "#   \"\"\"\n",
        "#   return None\n",
        "\n",
        "def standardize(data):\n",
        "  \"\"\"\n",
        "  Input: Pandas DataFrame\n",
        "  Output: Pandas DataFrame with mean == 0 and std == 1\n",
        "  \"\"\"\n",
        "  mean = np.mean(data, axis=0)\n",
        "  std = np.std(data, axis=0)\n",
        "  data_new = (data - mean) / std\n",
        "  return data_new\n",
        "\n",
        "def normalize(data):\n",
        "  \"\"\"\"\n",
        "  Input: Pandas DataFrame\n",
        "  Output: Pandas DataFrame with min == 0 and max == 1\n",
        "  \"\"\"\n",
        "\n",
        "  return (data - np.min(data, axis=0))/(np.max(data, axis=0) - np.min(data, axis=0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb4WgGQ0o_9G"
      },
      "source": [
        "Let's apply both functions separately and create X_standardized and X_normalized."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2qf7fOZG54MA"
      },
      "outputs": [],
      "source": [
        "X_standardized = standardize(X)\n",
        "X_normalized = normalize(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So46vKNl53KL"
      },
      "source": [
        "You may also need to check if those functions are correctly made. Create a function to check the dataset's min, max, mean, std of each feature. You can re-use your lab function (**describe**) but this time you are not allowed to use Pandas DataFrame. There is no expected format for this function if you are successfully able to plot four statistics (min, max, mean, std). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "l9BqVsG36Tsl"
      },
      "outputs": [],
      "source": [
        "def describe(data):\n",
        "  \"\"\"\n",
        "  Describe four statistics of the dataset.\n",
        "  \n",
        "  Input: NumPy ndarray\n",
        "  Output: vertical min, max, mean, standard deviation\n",
        "  \"\"\"\n",
        "  \n",
        "  print(\"Min: \", np.min(data, axis=0))\n",
        "  print(\"Max: \", np.max(data, axis=0))\n",
        "  print(\"Mean: \", np.mean(data, axis=0))\n",
        "  print(\"Std: \", np.std(data, axis=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZEPPjpJ6W3B"
      },
      "source": [
        "Using this function, let's check if your **standardize** and **normalize** functions are correctly working. \n",
        "- **Your output should be the same as the one below.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1NY6wtki6c84",
        "outputId": "64607448-a8c6-47ff-9851-1e953814f98b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min:  [-1.77429947 -2.19618048 -1.8523186  -1.61076772 -1.25612255 -0.22899997\n",
            " -1.447568   -2.38599234]\n",
            "Max:  [  5.85828581   1.85618152  55.16323628  69.57171326  30.25033022\n",
            " 119.41910319   2.95806762   2.62528006]\n",
            "Mean:  [ 6.60969987e-17  5.50808322e-18  6.60969987e-17 -1.06030602e-16\n",
            " -1.10161664e-17  3.44255201e-18 -1.07958431e-15 -8.52651283e-15]\n",
            "Std:  [1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "describe(X_standardized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7-p5vdl3H4F",
        "outputId": "4c845817-ac9f-47cb-f377-40d1aebac63e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min:  [0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Max:  [1. 1. 1. 1. 1. 1. 1. 1.]\n",
            "Mean:  [0.23246376 0.54195071 0.03248795 0.02262871 0.03986874 0.00191395\n",
            " 0.32857188 0.47612505]\n",
            "Std:  [0.13101721 0.24676966 0.01753907 0.0140484  0.03173953 0.00835784\n",
            " 0.226982   0.19955012]\n"
          ]
        }
      ],
      "source": [
        "describe(X_normalized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj47zZ264Az0"
      },
      "source": [
        "However, this is not a complete setting, as you need to both train the model and test it. That means you need to divide the dataset into two parts: {a training set, a test set} and only use the training set to train the model. This means that you also need to create the function for it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ORFET61FbZVy"
      },
      "outputs": [],
      "source": [
        "def train_test_split(X, y, test_ratio = 0.3):\n",
        "  # simulation\n",
        "  # cross-val\n",
        "  \n",
        "  \"\"\"\n",
        "  Input:\n",
        "    - X: a set of features\n",
        "    - y: corresponding labels\n",
        "    - test_ratio: ratio of the test set\n",
        "    \n",
        "  Output:\n",
        "    - X_train: separated training instances\n",
        "    - X_test: separated test instances\n",
        "    - y_train: separated training labels\n",
        "    - y_test: separated test labels\n",
        "  \n",
        "  1. Randomly shuffle the indices of the data instances\n",
        "  2. Divide the indices into two parts with the ratio of [1-test ratio:test ratio]\n",
        "  3. Select training instances and labels with the first set of indices and test instances and labels with the second set of indices\n",
        "  4. Return the training set and the test set\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  # Your test set size from train_test_split (6193) does not match the requirement (6192).\n",
        "\n",
        "\n",
        "\n",
        "  #Updated below\n",
        "  \n",
        "  indices = np.random.permutation(len(X))\n",
        "  test_size = int(len(X) * test_ratio)\n",
        "  test_indices, train_indices = indices[:test_size], indices[test_size:]\n",
        "\n",
        "  X_train, X_test = X[train_indices], X[test_indices]\n",
        "  y_train, y_test = y[train_indices], y[test_indices]\n",
        "  return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twOo6_yniUGO"
      },
      "source": [
        "Split your dataset into training and test sets with `test ratio = 0.3`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "f-iSb7IoijO1"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, 0.3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9PK6RGbkK7O"
      },
      "source": [
        "After applying your train_test_split function, you can check the shape of each subset. The training set should have 14,448 rows while the test set might have 6,192 records. Uncommend the below line and check the shapes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bk_pMMo-iquK",
        "outputId": "45b7d100-fe0f-4817-b39a-ad1703313b27"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((14448, 8), (6192, 8), (14448,), (6192,))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T8JBhsTj00O"
      },
      "source": [
        "You may remember, when you apply standardization or normalization on both training and test sets, you should not use any statistics from the test set. This means that you should use mean and standard deviation (or max and min values) of the training set and use those statistics to avoid cheating and make a valid model. \n",
        "\n",
        "- Create two functions (**apply_standardization**, **apply_normalization**) that uses training set's statistics and apply standardization or normalization to both sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XLs3P181kYHA"
      },
      "outputs": [],
      "source": [
        "def apply_standardization(X_train, X_test):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    - X_train: training instances\n",
        "    - X_test: test instances\n",
        "\n",
        "  Output:\n",
        "    - X_train_standardized\n",
        "    - X_test_standardized\n",
        "\n",
        "  Use training set's mean and standard deviation to standardize both training and test sets\n",
        "  \"\"\"\n",
        "\n",
        "  # Also your apply_standardization/apply_normalization functions are incorrect\n",
        "\n",
        "\n",
        "\n",
        "  #Updated below\n",
        "\n",
        "  mean = np.mean(X_train, axis = 0)\n",
        "  std = np.std(X_train, axis = 0)\n",
        "\n",
        "  X_train_standardized = (X_train - mean) /std\n",
        "  X_test_standardized = (X_test - mean) / std\n",
        "  \n",
        "  return X_train_standardized, X_test_standardized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sOx7Kd221jRq"
      },
      "outputs": [],
      "source": [
        "def apply_normalization(X_train, X_test):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    - X_train\n",
        "    - X_test\n",
        "\n",
        "  Output:\n",
        "    - X_train_standardized\n",
        "    - X_test_standardized\n",
        "  \"\"\"\n",
        "\n",
        "  # Also your apply_standardization/apply_normalization functions are incorrect\n",
        "\n",
        "\n",
        "\n",
        "  #Updated below\n",
        "  norm_train = np.linalg.norm(X_train, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "  X_train_standardized = X_train / norm_train\n",
        "\n",
        "\n",
        "  X_test_standardized = X_test / norm_train\n",
        "  return X_train_standardized, X_test_standardized"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujWQ84ZFCa10"
      },
      "source": [
        "- Apply two functions (**apply_standardization**, **apply_normalization**) to created standardized and normalized datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Cw8tjyoCtnqI"
      },
      "outputs": [],
      "source": [
        "X_train_standardized, X_test_standardized = apply_standardization(X_train, X_test)\n",
        "X_train_normalized, X_test_normalized = apply_normalization(X_train, X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efxhH-DvCxBx"
      },
      "source": [
        "Check the statistics using describe method. Test set should **NOT** have zero mean and standard deviation 1 or zero min and one max. Good test set however might show close value to zero or one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch5zQTDDDugQ",
        "outputId": "e54dd576-31d4-4650-8908-33e4a6d1f63a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min:  [-1.7579647  -2.19076287 -1.71769958 -1.46692518 -1.27018196 -0.22293089\n",
            " -1.44418114 -2.37641907]\n",
            "Max:  [  5.80282955   1.85525314  51.02675067  63.01310569  24.34623827\n",
            " 117.26138122   2.94970018   2.61572611]\n",
            "Mean:  [-1.20744285e-14  1.71966231e-16  5.30769385e-15 -3.80685835e-15\n",
            "  2.80514199e-17 -1.54434574e-17  2.28897128e-14  9.34617217e-14]\n",
            "Std:  [1. 1. 1. 1. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "describe(X_train_standardized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qyxhAClDx7e",
        "outputId": "2d0209e4-cc22-4f15-fba9-e4a391afa86c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min:  [-1.7579647  -2.19076287 -1.61140488 -1.14834795 -1.26838828 -0.19662112\n",
            " -1.43951177 -2.33166877]\n",
            "Max:  [ 5.80282955  1.85525314 21.3060368  24.8687923  30.72814655 56.41103143\n",
            "  2.94970018  2.5262255 ]\n",
            "Mean:  [-0.00118016  0.00659466 -0.01380141 -0.02608421  0.01850352  0.0064257\n",
            " -0.00158425  0.0015544 ]\n",
            "Std:  [0.96829138 0.99475512 0.72053049 0.63352341 1.05102433 0.93834808\n",
            " 0.9910778  0.98721896]\n"
          ]
        }
      ],
      "source": [
        "describe(X_test_standardized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tTsEozJ2_3a",
        "outputId": "a999a7a4-426b-401e-971f-5085a037dbb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min:  [ 9.62633679e-04  2.66071380e-04  1.16127141e-03  2.27538505e-03\n",
            "  1.38281610e-05  5.23217249e-04  7.58367623e-03 -8.65079334e-03]\n",
            "Max:  [ 0.02888498  0.01383571  0.19475769  0.23254435  0.13167175  0.93965942\n",
            "  0.00977674 -0.00795233]\n",
            "Mean:  [ 0.00745487  0.00761351  0.00746602  0.00751402  0.00654204  0.00230526\n",
            "  0.00830449 -0.0083183 ]\n",
            "Std:  [0.00369304 0.00335383 0.00367046 0.00357117 0.00513959 0.00799372\n",
            " 0.00049912 0.00013991]\n"
          ]
        }
      ],
      "source": [
        "describe(X_train_normalized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNplCpND3Byu",
        "outputId": "affd8964-32c1-4dbc-a8bc-844d481efc8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min:  [ 9.62633679e-04  2.66071380e-04  1.55142188e-03  3.41307758e-03\n",
            "  2.30469350e-05  7.33530064e-04  7.58600680e-03 -8.64453221e-03]\n",
            "Max:  [ 0.02888498  0.01383571  0.08566899  0.09632463  0.16447215  0.45323902\n",
            "  0.00977674 -0.00796485]\n",
            "Mean:  [ 0.00745052  0.00763563  0.00741536  0.00742087  0.00663714  0.00235663\n",
            "  0.0083037  -0.00831809]\n",
            "Std:  [0.00357594 0.00333624 0.00264468 0.00226242 0.00540184 0.00750089\n",
            " 0.00049467 0.00013812]\n"
          ]
        }
      ],
      "source": [
        "describe(X_test_normalized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgG7BAsDOx8B"
      },
      "source": [
        "### 2. Linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDLWBj56D6P6"
      },
      "source": [
        "Now you are ready to put your dataset to train a model. You will continue to use the linear regression that you have made in the lab using the normal equation. \n",
        "\n",
        "- Create the **solver** function that creates a linear regression line and return the coefficents. You can re-use the function from the first lab.\n",
        "- Here you should use **all available features** of the dataset.\n",
        "- You should add one column representing a bias to your feature matrix.\n",
        "\n",
        "The normal equation can be represented as follows:\n",
        "\n",
        "$\\theta = (\\textbf{X}^T \\cdot \\textbf{X})^{-1} \\cdot \\textbf{X}^T \\cdot \\textbf{y}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "6EkjV6APOx8C"
      },
      "outputs": [],
      "source": [
        "def solver(X, y):\n",
        "  \"\"\"\n",
        "  Get the weights and bias of linear regression classifier on the input dataset (X, y).\n",
        "\n",
        "  Input: \n",
        "   - X: a set of features\n",
        "   - y: labels\n",
        "  Output:\n",
        "   - theta: weights and bias of the linear regression\n",
        "  \"\"\"\n",
        "\n",
        "  X = np.array(X)\n",
        "  bias_feature = np.ones((X.shape[0], 1))\n",
        "  \n",
        "  X = np.concatenate((X, bias_feature), axis=1)\n",
        "  theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
        "  return theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWZhZW4rExeQ"
      },
      "source": [
        "You should run this solver function only on the standardized training set (**X_train_standardized**, **y_train**) to create the model and evalute it later on the test set.\n",
        "\n",
        "- Run the **solve** function on **X_train_standardized** and **y_train** and save the result to **theta**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_8ETLDgE_5p",
        "outputId": "b24a7b39-ede2-4bbe-e1d3-ef31aac41d07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.82629229  0.12168173 -0.2712592   0.31493921 -0.00375857 -0.04783266\n",
            " -0.9057927  -0.87920787  2.06490986]\n"
          ]
        }
      ],
      "source": [
        "theta = solver(X_train_standardized, y_train)\n",
        "print(theta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQDfYDDxF9o5"
      },
      "source": [
        "You now have a complete model trained on the training set. Then the next interesting thing is to evaluate if the model is good enough by using the test set. To do this, you need to create a predict function that can return the expected value. \n",
        "\n",
        "- Create the **predict** function which put each instance into the regression equation to predict the value. **DO NOT USE ANY LOOP**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "p-JxOI09ledN"
      },
      "outputs": [],
      "source": [
        "def predict(X, theta):\n",
        "  \"\"\"\n",
        "  Input: \n",
        "   - X: data instances to predict\n",
        "   - theta: trained regression coefficients\n",
        "\n",
        "  Output:\n",
        "   - y_hat: predicted values (X @ weight) + bias\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "  y_hat = np.dot(X, theta[:-1]) + theta[-1]\n",
        "  return y_hat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "psp-L1wGRslu"
      },
      "source": [
        "This predict function should be able to return the predicted value of the housing price. Then now you might want to return the mean squared error (and its variants) of the whole model. There can be many different metrics but here you will measure the rooted mean squared error (RMSE). RMSE can be calculated as follows: "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljB3uUqjWPbw"
      },
      "source": [
        "$RMSE = \\sqrt{\\frac{1}{n}\\sum_{t=1}^{n}(\\hat{y}_t - y_t)^2} $.\n",
        "\n",
        "Note that $\\hat{y}$ is a predicted label and $y$ is a true label."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMTnVQGPWfBv"
      },
      "source": [
        "- Create a function **rooted_mean_squared_error** that calculates the RMSE value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "laAN9bTMOx8C"
      },
      "outputs": [],
      "source": [
        "def rooted_mean_squared_error(X, y, theta):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    - X_test: data instances to test\n",
        "    - y_test: true class labels of the corresponding data instances (X_test)\n",
        "    - theta: trained regression coefficients\n",
        "\n",
        "  Output:\n",
        "    - RMSE: the RMSE score\n",
        "\n",
        "  Use predict function to calculate our predicted values.\n",
        "  \"\"\"\n",
        "  \n",
        "  predictions = predict(X,theta)\n",
        "  rmse = np.sqrt(np.sum(np.square(np.subtract(predictions,y)))/len(X))\n",
        "  return rmse\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TReu0cnIWiz2"
      },
      "source": [
        "- Run RMSE function to test our model created by our solver function. Use predict function to calculate our predicted values. Report the RMSE value. Since you used the standardized dataset for training, you should do the same for testing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52aH-ML6Seu7"
      },
      "source": [
        "Even though the RMSE is generally the preferred performance measure for\n",
        "regression tasks, in some contexts you may prefer to use another function. For\n",
        "example, suppose that there are many outliers. In that case, you may\n",
        "consider using the mean absolute error (MAE). It's direct translation of l1 and l2 norm. The higher the norm index, the more it focuses on large values and\n",
        "neglects small ones. This is why the RMSE is more sensitive to\n",
        "outliers than the MAE. But when outliers are exponentially rare (like\n",
        "in a bell-shaped curve), the RMSE performs very well and is\n",
        "generally preferred.\n",
        "\n",
        "MAE can be calculated as follows:\n",
        "\n",
        "$MAE = \\frac{1}{n}\\sum_{t=1}^{n}|\\hat{y}_t - y_t|$\n",
        "\n",
        "- Implement a function for MAE **mean_absolute_error**, which receives the same parameters *X*, *y*, and *theta*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "uFEGAF7fqqeg"
      },
      "outputs": [],
      "source": [
        "def mean_absolute_error(X, y, theta):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    - X_test: data instances to test\n",
        "    - y_test: true values of the corresponding data instances (X_test)\n",
        "    - theta: trained regression coefficients\n",
        "\n",
        "  Output:\n",
        "    - MAE: MAE score\n",
        "\n",
        "  Use predict function to calculate our predicted values.\n",
        "  \"\"\"\n",
        "  \n",
        "\n",
        "  predictions = predict(X, theta)\n",
        "  return np.mean(np.abs(y - predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNC0hszHqyIP"
      },
      "source": [
        "Train your regression model on the **standardized** training set and evaluate your method with two different scores: RMSE and MAE. Print two scores here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "wm8gHjCSq64P"
      },
      "outputs": [],
      "source": [
        "rmse_score = rooted_mean_squared_error(X_test_standardized, y_test, theta) # CHANGE IT!\n",
        "mae_score = mean_absolute_error(X_test_standardized, y_test, theta) # CHANGE IT!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWaJtiItrmDk",
        "outputId": "11d63b7f-5bd8-4a72-9a15-b579ef487714"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(0.7333577746953766, 0.5377716992433311)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rmse_score, mae_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "royxfFHjRjw2"
      },
      "source": [
        "### 4. Linear regression with regularization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wphcDNvvkwL0"
      },
      "source": [
        "You have learned the Ridge regression in the lecture. Fortunately, the Ridge regression also can be represented as a closed form solution with the normal equation. \n",
        "\n",
        "Your task here is to create a variant of your previous solver function supporting the Ridge regression. \n",
        "\n",
        "A closed form solution to Ridge can be represented as follows:\n",
        "\n",
        "$\\theta = (\\textbf{X}^T \\cdot \\textbf{X} + \\lambda \\textbf{I})^{-1} \\cdot \\textbf{X}^T \\cdot \\textbf{y}$\n",
        "\n",
        "where $\\textbf{I}$ is an $(n+1) \\times (n+1) $ identity matrix, since the feature matrix also includes the bias column.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "jvYlCgotRlcK"
      },
      "outputs": [],
      "source": [
        "def solver_with_ridge(X, y, alpha):\n",
        "  \"\"\"\n",
        "  Get the weights and bias of the linear regression line on the dataset X, using the labels y.\n",
        "\n",
        "  Input: \n",
        "   - X: a set of features to get weights\n",
        "   - y: class labels\n",
        "  Output:\n",
        "   - theta: weights and bias of the ridge regression\n",
        "  \"\"\"\n",
        "\n",
        "  intercept_ones = np.ones((len(X_train), 1))\n",
        "  X = np.c_[X, intercept_ones] \n",
        "  I = np.identity(X.shape[1]) \n",
        "  I[0][0] = 1\n",
        "  theta = np.linalg.inv(X.T.dot(X) + alpha * I).dot(X.T).dot(y)\n",
        "\n",
        "  return theta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-LrDqWSmaGW"
      },
      "source": [
        "Here, compare the performances changing the $\\lambda$ value. Use the $\\lambda$ value from 0 to 30 in increments of 0.1. Use RMSE as a score metric. Save those 300 scores into the list `scores`. \n",
        "\n",
        "- To iterate different $\\lambda$s, you can use a loop for your convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Vd9pJ8zUo_7u"
      },
      "outputs": [],
      "source": [
        "# For the plot of solver_with_ridge, you did not save the RMSE scores but the theta values from differemt lambdas so the plot is not as desired.\n",
        "\n",
        "\n",
        "\n",
        "#Updated below\n",
        "rmse_scores = []\n",
        "\n",
        "for alpha in np.arange(0, 30, 0.1):\n",
        "    theta = solver_with_ridge(X_train_standardized, y_train, alpha)\n",
        "    rmse = rooted_mean_squared_error(X_test_standardized, y_test, theta)\n",
        "    rmse_scores.append(rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILjT1k5FpLVe"
      },
      "source": [
        "Plot the graph of different scores here. If you saved all scores in the list `scores`, you can simply run the block below. The resulting plot behaves in a different way based on your split training and test sets. Sometimes, the error just decreases or increases, but you can also see that the error decreases first, but after some point, it starts to increase. If you are interested, repeat many times to check different plots and you can even change the range from [0, 30] to something else. Uncomment the block below!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "--Ky0LHO0wqM",
        "outputId": "ad303a4c-ab34-43f3-879f-220f1439bc65"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(np.arange(0, 30, 0.1), rmse_scores)\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"RMSE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3nHFnagh0nC"
      },
      "source": [
        "### 5. Model validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7MnVQ5Rryof"
      },
      "source": [
        "So far, you simply had one test set and one training set. Now the question is if those sets were enough to represent the dataset's distribution. To overcome this problem, various validation methods have been developed and used such as cross-validation or a repeated holdout test. Here, you will develop one function that performs the repeated holdout test. The key strategy of it is to create a completely different training and test set pair for each iteration. You need to iterate the holdout test that you performed many (k) times and return the average score.\n",
        "\n",
        "* You are allowed to use a for loop to iterate k different tests. However, you are not allowed to use the loop to create different indices to divide the dataset.\n",
        "* You can call the `train_test_split` function you have developed above if it helps your development process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "M7JEfZjGijfH"
      },
      "outputs": [],
      "source": [
        "def repeated_hold_out(X, y, k, test_ratio):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    - X: features\n",
        "    - y: labels\n",
        "    - test_ratio: ratio of the test set\n",
        "  Output:\n",
        "    - score: the average of k different test scores\n",
        "  \n",
        "  1. Iterate k times to perform k validation processes.\n",
        "  2. For each iteration, split the dataset into training and test sets with *random* indices.\n",
        "   - Note that each iteration should create different training and test sets.\n",
        "  3. Use *standardization* to fix the scale of the dataset, you should only use the training set's properties.\n",
        "  4. Fit your model with *solver* (without ridge) on the training set.\n",
        "  5. Save your *RMSE* score into the list *scores*\n",
        "  6. After all the iterations, return the average of *scores*.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  scores = []\n",
        "  for i in range(k):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_ratio)\n",
        "    X_train_standardized, X_test_standardized = apply_standardization(X_train, X_test)\n",
        "    theta = solver(X_train_standardized, y_train)\n",
        "    rmse_score = rooted_mean_squared_error(X_test_standardized, y_test, theta)\n",
        "    scores.append(rmse_score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return np.mean(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po5UX-gb2idF"
      },
      "source": [
        "Run the line below to return your holdout score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuMGeSGfLjAL",
        "outputId": "1b51cf58-53f4-441b-f70d-5ce57abc6d6e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7323647509969161"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "holdout_score = repeated_hold_out(X, y, k=5, test_ratio=0.2)\n",
        "holdout_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7mSIvYbRoQq"
      },
      "source": [
        "### 6. Put things together"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWjft22LHEJm"
      },
      "source": [
        "It's time to put everything you have done together here. Here you will create a function that manages whole process from receiving raw datasets to returning performance metrics, by modifying the repeated holdout function. This will help you manage your process clearly since it must contain all the functions you use for your dataset (Later you will replace it with scikit-learn's pipeline technique for the same purpose) - By having these management functions, you can switch off some of the techniques, add more techniques in the middle, or replace some of them with other methods, without any problem or confusion.\n",
        "\n",
        "* Complete `pipeline` following the instruction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "6CU_kV0DRpYg"
      },
      "outputs": [],
      "source": [
        "def pipeline(X, y, k = 5, test_ratio = 0.2, norm_method = \"standardization\", eval_method = \"RMSE\", alpha = 0):\n",
        "  \"\"\"\n",
        "  Input:\n",
        "    - X: features\n",
        "    - y: labels\n",
        "    - test_ratio: ratio of the test set\n",
        "  Output:\n",
        "    - score: the average of k different test scores\n",
        "  \n",
        "  1. Iterate k times to perform k validation processes.\n",
        "  2. For each iteration, split the dataset into the training and test sets with *random* indices.\n",
        "   - Note that each iteration should create different training and test sets.\n",
        "  3. Check the parameter *norm_method*\n",
        "    - if norm_method == standardization:\n",
        "      - Use *standardization* to fix the scale of the dataset, you should only use the training set's properties.\n",
        "    - if norm_method == normalization:\n",
        "      - Use *normalization* to fix the scale of the dataset, you should only use the training set's properties.\n",
        "  4. Fit your model with *solver_with_ridge\" on the training set. Use alpha from the parameter.\n",
        "  5. Check the parameter \"eval_method\"\n",
        "    - if eval_method == \"RMSE\"\n",
        "      - Save your *RMSE* score into the list *scores*\n",
        "    - if eval_method == \"MAE\"\n",
        "      - Save your *MAE* score into the list *scores*\n",
        "\n",
        "  6. After all the iterations, return the average of *scores*.\n",
        "\n",
        "  \"\"\"\n",
        "  scores = []\n",
        "  for i in range(k):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_ratio)\n",
        "    if norm_method == \"standardization\":\n",
        "      X_train_st, X_test_st = apply_standardization(X_train, X_test)\n",
        "    if norm_method == \"normalization\":\n",
        "      X_train_st, X_test_st = apply_normalization(X_train, X_test)\n",
        "    theta = solver_with_ridge(X_train_st, y_train,alpha)\n",
        "    if eval_method == \"RMSE\":\n",
        "      rmse_score = rooted_mean_squared_error(X_test_st, y_test, theta)\n",
        "      scores.append(rmse_score)\n",
        "    if eval_method == \"MAE\":\n",
        "      mae_score = mean_absolute_error(X_test_st, y_test, theta)\n",
        "      scores.append(mae_score)\n",
        "\n",
        "\n",
        "  return np.mean(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7GKrthoLOlD"
      },
      "source": [
        "Now you are ready to run various tasks by using this single function. Will the best model the same under RMSE or MAE? Will different k or test ratio result in different best model? You can do many different trials to find a good model.\n",
        "\n",
        "- (Optional) Change a normalization method, an alpha value to find out the best classifier under either RMSE or MAE score. This task is completely optional and will not affect your homework grade."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "bkCeCDoPLNGp"
      },
      "outputs": [],
      "source": [
        "best_model = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWugHCgB2Cpq"
      },
      "source": [
        "# END"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
